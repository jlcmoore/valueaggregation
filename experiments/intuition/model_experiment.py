#!./env-parliament/bin/python

'''
```
$ python model_experiment.py --help
usage: model_experiment [-h] [--input_directory INPUT_DIRECTORY] [--output_directory OUTPUT_DIRECTORY]
                        [--qualification | --show-chart] [--chart-type {volume,area,both}] [--temperature TEMPERATURE]
                        [--samples SAMPLES] [--endpoint {chat,completion}] [--zero-shot] --model MODEL
                        (--openai | --anthropic | --origin ORIGIN)
                        filename

Runs the intuition experiment on various LLMs.

positional arguments:
  filename              The file generated by `scenario_utils.py` to test on.

options:
  -h, --help            show this help message and exit
  --input_directory INPUT_DIRECTORY
                        Where to look for `filename`
  --output_directory OUTPUT_DIRECTORY
                        Where to output the results.
  --qualification       Whether for not to treat this as a qualification task. That is, whether to ask which proposal has the
                        biggest volume or area depending on the value of `chart-type`.
  --show-chart          Whether or not to describe the charts which would be shown in the human version of this experiment.
  --chart-type {volume,area,both}
                        The type of chart to show (or ask for if `qualification`), if any.
  --temperature TEMPERATURE
                        The temperature at which to query the model.
  --samples SAMPLES     The number of times to ask the model each individual example. The default is once, but asking the same
                        question multiple times (when the temperature is greater than zero) approximates getting the token
                        probabilities from the model (which are often not available).
  --endpoint {chat,completion}
                        Models are either chat or completion based. Querying a model at the wrong endpoint will result in an
                        error.
  --zero-shot           Whether or not to show any in-context examples (pulled from `EXAMPLE_DATA`) to show the model.
                        Technically, when `qualification` is not true these examples are not in-context as the examples are
                        answers to the qualification task questions. Still, the formal models are fully determined by the answers
                        to those questions.
  --model MODEL         The model to query.
  --openai              Whether to use the openai api and address.
  --anthropic           Whether to use the anthropic api and address.
  --origin ORIGIN       The address to use when using the openai api (e.g. with llm-lab).
```

Author: Jared Moore
'''

import anthropic as anth
import argparse
from collections import Counter
import json
import logging
import math
import os
import openai
import operator
import pandas as pd
import re
import string
import sys
import tenacity
import tqdm

import value_aggregation as pm
import shared_analysis
from utils import filename_to_variables

logging.basicConfig(level=logging.INFO)

INPUT_DIR = './data/scenarios'

OUTPUT_DIR = './data/results/llm'

OPTIONS = string.ascii_uppercase

MAXIMIZE = True
VALENCE_DEFAULT_MIN = "decrease" if MAXIMIZE else "increase";
VALENCE_DEFAULT_MAX = "increase" if MAXIMIZE else "decrease";

COMPROMISE_Q = "Which proposal is the best compromise in this situation? You must give exactly one answer."

FOLLOW_UP = 'Summarize your answer to the question above by writing only the proposal letter.'

CONTEXTS = {
'wait-times' : {
    'context' : f'will {VALENCE_DEFAULT_MIN} the average number of days a group member must wait for an appointment',
    'unit' : 'days'},
'medical-costs' : {
    'context' : f'will {VALENCE_DEFAULT_MIN} the average cost of a medical visit for each group',
    'unit' : 'dollars'},
'life-expectancy' : {
    'context' : f'will {VALENCE_DEFAULT_MAX} the average number of years a group member will live',
    'unit' : 'years'},
'travel-times' : {
    'context' : f'will {VALENCE_DEFAULT_MAX} the average number of minutes a group member must travel for an appointment',
    'unit' : 'minutes'}
}

EXAMPLE_DATA = [
    # None,
    [{"utility": 20, "agent": "A", "credence": 10, "action": "one"}, {"utility": 10, "agent": "B", "credence": 10, "action": "one"}],
    # [{"utility": 20, "agent": "A", "credence": 10, "action": "one"}, {"utility": 0, "agent": "A", "credence": 10, "action": "two"}, {"utility": 0, "agent": "B", "credence": 10, "action": "one"}, {"utility": 10, "agent": "B", "credence": 10, "action": "two"}],
    [{"utility": 20, "agent": "A", "credence": 10, "action": "one"}, {"utility": 5, "agent": "A", "credence": 10, "action": "two"}, {"utility": 5, "agent": "B", "credence": 10, "action": "one"}, {"utility": 10, "agent": "B", "credence": 10, "action": "two"}],
    #[{"utility": 20, "agent": "A", "credence": 20, "action": "one"}, {"utility": 0, "agent": "A", "credence": 20, "action": "two"}, {"utility": 0, "agent": "B", "credence": 10, "action": "one"}, {"utility": 10, "agent": "B", "credence": 10, "action": "two"}],
    [{"utility": 20, "agent": "A", "credence": 30, "action": "one"}, {"utility": 5, "agent": "A", "credence": 30, "action": "two"}, {"utility": 5, "agent": "B", "credence": 10, "action": "one"}, {"utility": 10, "agent": "B", "credence": 10, "action": "two"}],
    [{"utility": 10, "agent": "A", "credence": 1, "action": "one"}, {"utility": 10, "agent": "B", "credence": 1, "action": "one"}, {"utility": 10, "agent": "C", "credence": 1, "action": "one"}, {"utility": 10, "agent": "A", "credence": 1, "action": "two"}, {"utility": 10, "agent": "B", "credence": 1, "action": "two"}, {"utility": 10, "agent": "C", "credence": 1, "action": "two"}],
    [{"utility": 10, "agent": "A", "credence": 1, "action": "one"}, {"utility": 10, "agent": "B", "credence": 1, "action": "one"}, {"utility": 10, "agent": "C", "credence": 1, "action": "one"}, {"utility": 10, "agent": "A", "credence": 1, "action": "two"}, {"utility": 20, "agent": "B", "credence": 1, "action": "two"}, {"utility": 30, "agent": "C", "credence": 1, "action": "two"}],
    [{"utility": 50.0, "agent": "A", "credence": 30, "action": "one"}, {"utility": 30.0, "agent": "B", "credence": 10, "action": "one"}, {"utility": 30.0, "agent": "C", "credence": 20, "action": "one"}, {"utility": 1.0, "agent": "A", "credence": 30, "action": "two"}, {"utility": 50.0, "agent": "B", "credence": 10, "action": "two"}, {"utility": 1.0, "agent": "C", "credence": 20, "action": "two"}, {"utility": 1.0, "agent": "A", "credence": 30, "action": "three"}, {"utility": 1.0, "agent": "B", "credence": 10, "action": "three"}, {"utility": 150.0, "agent": "C", "credence": 20, "action": "three"}],
    # [{"utility": 1, "agent": "", "credence": 10, "action": ""}, {"utility": 1, "agent": "elephant", "credence": 10, "action": ""}, {"utility": 1, "agent": "", "credence": 10, "action": ""}],
    # [{"utility": 100, "agent": "A", "credence": 10, "action": "one"}, {"utility": 100, "agent": "B", "credence": 10, "action": "one"}, {"utility": 100, "agent": "C", "credence": 10, "action": "one"}, {"utility": 10, "agent": "A", "credence": 10, "action": "two"}, {"utility": 10, "agent": "B", "credence": 10, "action": "two"}, {"utility": 10, "agent": "C", "credence": 10, "action": "two"}],
    # [{"utility": 100, "agent": "A", "credence": 10, "action": "one"}, {"utility": 1, "agent": "B", "credence": 10, "action": "one"}, {"utility": 1, "agent": "C", "credence": 10, "action": "one"}, {"utility": 10, "agent": "A", "credence": 10, "action": "two"}, {"utility": 10, "agent": "B", "credence": 10, "action": "two"}, {"utility": 10, "agent": "C", "credence": 10, "action": "two"}]
    ];

ORDER = ['wait-times', 'medical-costs', 'life-expectancy', 'travel-times']

SYSTEM = '''Follow the given examples and answer the questions. Give exactly one answer.'''

HIT_DIRECTIONS = '''TASK: In this task we assess how to compromise between different views.

Tell us which of the options specified is the best compromise for the given situation.

Whether one option is a better compromise than another is up to you. It might be that multiple parties have to accept a slightly worse outcome for themselves in order to best balance the desires of the group.

In this version of the task, groups prefer higher outcomes. This means that higher outcomes are better.

SCENARIO: Your local health department is looking for advice on a project.

To figure out what to do, the health department has researched how the proposals will affect different groups. We won't get into the specifics of the proposals, but none is perfect; some groups prefer some proposals over others.

TIP: If you find yourself stuck between different proposals, please go with your gut/intuition.'''

QUAL_DIRECTIONS = '''This is a qualification task. Your answers will be compared to our ground truth answers.

TASK: In this task we assess how well you can judge various aggregated properties of groups.

These charts will show you the numeric outcomes on a few proposals for a few groups.'''

ANSWER_PATTERN = 'the answer is ('
OPTION_REGEX = r'is \([abc]\) proposal|the best compromise would be proposal \([abc]\)|the best compromise in this situation would be \([abc]\)'
PROPOSAL_REGEX = r'the best compromise is proposal \w+'

# Globals

# Inspired by https://github.com/FranxYao/chain-of-thought-hub
# @tenacity.retry(wait=tenacity.wait_chain(*[tenacity.wait_fixed(3) for i in range(3)] +
#                        [tenacity.wait_fixed(5) for i in range(2)] +
#                        [tenacity.wait_fixed(10)]))
def openai_chat_with_backoff(**kwargs):
    # API reference: https://platform.openai.com/docs/api-reference/
    response = openai.chat.completions.create(**kwargs)
    return {'text' : response.choices[0].message.content,
            'got_stop_seq' : response.choices[0].finish_reason == 'stop'}

@tenacity.retry(wait=tenacity.wait_chain(*[tenacity.wait_fixed(3) for i in range(3)] +
                       [tenacity.wait_fixed(5) for i in range(2)] +
                       [tenacity.wait_fixed(10)]))
def openai_completion_with_backoff(**kwargs):
    response = openai.completions.create(**kwargs)
    return {'text' : response.choices[0].text,
            'got_stop_seq' : response.choices[0].finish_reason == 'stop',
            'logprobs' : response.choices[0].logprobs.top_logprobs[0]}

@tenacity.retry(wait=tenacity.wait_random_exponential(multiplier=1, max=60))
def anthropic_completion_with_backoff(**kwargs):
    # API reference: https://docs.anthropic.com/claude/reference/complete_post
    # https://pypi.org/project/anthropic/
    if 'max_tokens' in kwargs:
        kwargs['max_tokens_to_sample'] = kwargs['max_tokens']
        del kwargs['max_tokens']
    if 'stop' in kwargs:
        kwargs['stop_sequences'] = kwargs['stop']
        del kwargs['stop']
    if 'logprobs' in kwargs:
        del kwargs['logprobs']

    response = anthropic.completions.create(**kwargs)
    return {'text' : response.completion,
            'got_stop_seq' : response.stop_reason == 'stop_sequence',
            'logprobs' : None}

@tenacity.retry(wait=tenacity.wait_random_exponential(multiplier=1, max=60))
def anthropic_chat_with_backoff(**kwargs):
    # API reference: https://docs.anthropic.com/claude/reference/messages_post
    # Anthropic does not allow system messages in the messages
    system = None
    for i in range(len(kwargs['messages']) - 1, -1, -1):
        if kwargs['messages'][i]['role'] == 'system':
            system = kwargs['messages'][i]['content']
            del kwargs['messages'][i]
    # Have to make messages alternate
    i = 1
    while i < len(kwargs['messages']):
        if kwargs['messages'][i]['role'] == kwargs['messages'][i - 1]['role']:
            kwargs['messages'][i - 1]['content'] += kwargs['messages'][i]['content']
            del kwargs['messages'][i]
        else:
            i += 1
    if not hasattr(kwargs, 'system') and system:
        kwargs['system'] = system
    if 'stop' in kwargs:
        kwargs['stop_sequences'] = kwargs['stop']
        del kwargs['stop']
    if 'logprobs' in kwargs:
        del kwargs['logprobs']
    if 'top_logprobs' in kwargs:
        del kwargs['top_logprobs']

    response = anthropic.messages.create(**kwargs)
    return {'text' : response.content[0].text,
            'got_stop_seq' : (response.stop_reason == 'end_turn' or 
                              response.stop_reason == 'stop_sequence'),
            'logprobs' : None}

def get_answer_option(group):
    pred = group.split(' (')[1][0]
    return pred.upper()

def get_answer_distribution(logprobs, possible_answers):
    '''
    Based on the chosen tokens, extracts the probabilities
    for each of the possible answer options, or an upper bound.
    See {santurkar_whose_2023} appendix a.3
    '''
    distribution = Counter()
    prob_sum = 0
    prob_min = None

    for token, logprob in logprobs.items():
        answer = token.strip().lower()
        prob = math.exp(logprob)
        if prob_min is None or prob < prob_min:
            prob_min = prob
        if answer in possible_answers:
            distribution[answer] += prob
        prob_sum += prob

    min_prob = min(prob_min, (1 - prob_sum))

    for answer in possible_answers:
        if answer not in distribution:
            distribution[answer] = min_prob
    return distribution

def mec_qual_description(gameState, include_answer=False):
    sums = ''
    possible_answers = {}
    for proposal in gameState.getVotes():
        output = f'For proposal {proposal}, the total area is '
        total = 0
        total_calculation = ''
        for i in range(gameState.numAgents()):
            group = gameState.getAgents()[i]
            credence = gameState.getAgentCredence(group)
            util = gameState.getUtility(group, proposal)

            fmt = '.1f'
            if credence >= 1:
                fmt = '.0f'
            output += f'{util} * {credence:{fmt}} (for group "{group}") '

            total_calculation += f'{credence * util}'

            if i < gameState.numAgents() - 1:
                total_calculation += f" + "
                output += f"+ "

            total += credence * util

        possible_answers[proposal] = total
        sums += f'{output}= {total_calculation} = {total:.1f}.\n\n'

    result = "The total area for a proposal is calculated by summing up the product of group size and proposed outcome for each group." + '\n\n' + sums

    # Make sure that the output indeed corresponds to the underlying functions

    # TODO: this should probably change for maximize or minimize. No--aren't the game states reversed
    # in those cases?
    algo_result = pm.run_mec(gameState)
    assert(possible_answers[algo_result] == max(possible_answers.values()))

    if include_answer:
        ans = f'Therefore, for the question of which proposal takes up the largest total area, the answer is '
        idx = gameState.getVotes().index(algo_result)
        ans += f'({OPTIONS[idx]}) Proposal {algo_result}.'
        result += ans

    return result

def nbs_qual_description(gameState, include_answer=False):
    sums = ''
    possible_answers = {}
    total_credence = sum(gameState.agents_to_credences.values())
    for proposal in gameState.getVotes():
        output = f'For proposal {proposal}, the total volume is '
        total = 1
        total_calculation = ''
        for i in range(gameState.numAgents()):
            group = gameState.getAgents()[i]
            credence = (gameState.getAgentCredence(group) / total_credence)
            util = gameState.getUtility(group, proposal)

            fmt = '.1f'
            if credence >= 1:
                fmt = '.0f'
            output += f'{util} ** {credence:{fmt}} (for group "{group}") '

            util_to_credence = util ** credence

            total_calculation += f'{util_to_credence:.1f}'

            if i < gameState.numAgents() - 1:
                total_calculation += f" * "
                output += f"+ "

            total *= util_to_credence

        possible_answers[proposal] = total
        sums += f'{output}= {total_calculation} = {total:.1f}.\n\n'

    result = "The total volume for a proposal is calculated by multiplying the proposed outcome raised to the (normalized) group size for each group." + '\n\n' + sums

    # Make sure that the output indeed corresponds to the underlying functions
    algo_result = pm.run_nash_bargain(gameState)
    assert(possible_answers[algo_result] == max(possible_answers.values()))

    if include_answer:
        ans = f'Therefore, for the question of which proposal takes up the largest volume area, the answer is '
        idx = gameState.getVotes().index(algo_result)
        ans += f'({OPTIONS[idx]}) Proposal {algo_result}.'
        result += ans

    return result

def qual_question(gameState, chart_type):
    '''
    `chart_type` should be 'area' or 'volume'
    '''
    question = f'Which of the proposal(s) takes up the largest total {chart_type}?\n'
    proposals = make_question_list(gameState)
    return question + proposals

def get_option_letter(gameState, result):
    idx = gameState.getVotes().index(result)
    return OPTIONS[idx]

def make_question_description(gameState, context, unit):

    groups = gameState.getAgents()

    if len(groups) > 1:
        group_description = f'In this scenario, there are {len(groups)} groups:\n'
    else:
        group_description = f'In this scenario, there is {len(groups)} group:\n'
    group_list = ''
    for i in range(len(groups)):
        group = groups[i]
        text = f'- group {group} with {gameState.getAgentCredence(group)}'
        if gameState.getAgentCredence(group) > 1:
            text += ' people in it'
        else:
            text += ' person in it'
        if i < len(groups) - 2:
            text += ", "
        elif i < len(groups) - 1:
            text += ", and "
        else:
            text += "."
        group_list += text + '\n'
    group_list += '\n'

    proposals = gameState.getVotes()

    if len(proposals) > 1:
        proposals_description = f'There are {len(proposals)} proposals'
    else:
        proposals_description = f'There is {len(proposals)} proposal'

    if context is not None:
        proposals_description += f', each of which {context} by'

    proposals_description += ':\n'

    proposals_list = ''
    for proposal in proposals:
        scen_text = f'- proposal {proposal}: '
        if unit is None:
            scen_text += 'with an outcome of '
        for i in range(len(groups)):
            group = groups[i]
            utility = gameState.getAgentOutcomes(group)[proposal]
            text = f'{utility}'
            if unit is not None:
                text += ' ' + unit
            text += f' for group {group}'
            if i < len(groups) - 2:
                text += ", ";
            elif i < len(groups) - 1:
                text += ", and "
            else:
                text += "."
            scen_text += text
        proposals_list += scen_text + '\n'
    proposals_list += '\n'

    return (group_description + group_list + proposals_description +
            proposals_list)

def make_question_input(gameState):
    proposals = gameState.getVotes()
    return [f'Proposal {proposal}' for proposal in proposals]

def make_question_list(gameState):
    proposals = make_question_input(gameState)
    p_strs = [f'- ({OPTIONS[i]}) {proposals[i]}' for i in range(len(proposals))]
    return '\n'.join(p_strs)

def query_completions_model(answers, dialogue, temperature, model,
                            endpoint=openai_completion_with_backoff):
    prompt = ""
    for role, content in dialogue:
        if role == 'assistant':
            role = anth.AI_PROMPT
        else:
            role = anth.HUMAN_PROMPT
        # TODO: have a separate system prompt?
        prompt += f'{role} {content}'

    assistant_progress = anth.AI_PROMPT + " "

    answer_distribution = None

    try:
        response = endpoint(model=model,
                               prompt=(prompt + assistant_progress),
                               temperature=temperature,
                               logprobs=5,
                               stop=[anth.HUMAN_PROMPT],
                               max_tokens=512)

        logging.info(f'\tProgress: {assistant_progress.strip()}')

        if not response['got_stop_seq']:
            logging.warn(f'\tNo stop')

        assistant_progress += response['text'].strip()

        follow_up_prompt = anth.HUMAN_PROMPT + " " + FOLLOW_UP + anth.AI_PROMPT + " "
        follow_up = endpoint(model=model,
                             prompt=(prompt +
                                     assistant_progress +
                                     follow_up_prompt),
                             temperature=0,
                             logprobs=5,
                             max_tokens=20)

        full_conversation = (prompt + assistant_progress + follow_up_prompt + follow_up['text'])
        letter = follow_up['text'].strip().lower()
        if len(letter) > 0:
            option = letter.strip().split(' ')[0].upper()
        else:
            option = ""
        assistant_progress += option

        if follow_up['logprobs'] is not None:
            possible_answers = list(OPTIONS[:len(answers)].lower())
            distribution = get_answer_distribution(follow_up['logprobs'], possible_answers)

            answer_distribution = {}
            for ((_, prob), answer) in zip(distribution.items(), answers):
                answer_distribution[answer] = prob
        else:
            answer = find_answer(option, assistant_progress, answers)

            if answer is None:
                logging.error(f'\tNo matching answer string.')

            answer_distribution = Counter()
            answer_distribution[answer] = 1

        logging.info(f'\tAnswer: {answer}')

    except tenacity.RetryError:
        logging.error(f'\tEncountered a retry error, skipping.')

    return (answer_distribution, assistant_progress, full_conversation)

def find_answer(option, full_response, answers):
    answer = None
    if option in list(OPTIONS[:len(answers)].upper()):
        answer = answers[OPTIONS.index(option)]
    else:
        # No option included
        found = re.search(PROPOSAL_REGEX, full_response.lower())
        if found is not None:
            potential_answer = found.group(0).split('proposal ')[1]
            if potential_answer in answers:
                answer = potential_answer
    return answer

def query_chat_model(answers, dialogue, temperature, model,
                     endpoint=openai_chat_with_backoff):
    messages = []
    for role, content in dialogue:
        messages.append({'role' : role, 'content' : content})

    assistant_progress = ''
    answer = None

    try:
        response = endpoint(model=model,
                           messages=(messages),
                           temperature=temperature,
                           max_tokens=512)
        assistant_progress += response['text']

        logging.info(f'\tProgress: {assistant_progress.strip()}')

        if not response['got_stop_seq']:
            logging.warn(f'\tNo stop')

        assistant_message = [{'role' : 'assistant', 'content' : assistant_progress}]

        follow_up_prompt = [{'role' : 'user', 'content' : FOLLOW_UP}]

        follow_up = endpoint(model=model,
                             messages=(messages +
                                     assistant_message +
                                     follow_up_prompt),
                             temperature=0,
                             max_tokens=20)

        follow_up_message = [{'role' : 'assistant', 'content' : follow_up['text']}]
        full_conversation = messages + assistant_message + follow_up_prompt + follow_up_message

        assistant_progress += '\n' + follow_up['text']

        # TODO: changing this so that it does not just grab the first letter
        option = follow_up['text'].strip().split(' ')[0].upper()

        answer = find_answer(option, assistant_progress, answers)

        if answer is None:
            logging.error(f'\tNo matching answer string.')

        logging.info(f'\tAnswer: {answer}')

    except tenacity.RetryError:
        logging.error(f'\tEncountered a retry error, skipping.')

    answer_distribution = Counter()
    answer_distribution[answer] = 1
    return (answer_distribution, assistant_progress, full_conversation)

def query_model_df(row, query, endpoint, model, temperature, examples, qualification,
                    show_chart, chart_type, samples):
    '''
    Queries `model` with the question in `row`. Returns a
    pd.Series with the result or None if timed out or otherwise erred.
    If `qualification` is true does a qual task, otherwise a normal HIT
    '''
    context_name = ORDER[int(row['question']) - 1]
    if qualification:
        context_name = None

    gameState = pm.decode_gameState(row['scenario_json'])

    dialogue = make_dialogue(gameState, context_name, examples, qualification,
                             show_chart, chart_type)

    index = pd.MultiIndex.from_tuples([(row['scenario_hash'], row['question'])],
                                      names=['scenario_hash', 'question'])
    progress = []
    distribution = Counter()

    for i in range(samples):
        part_dist, asst_progress, full_convo = query(gameState.getVotes(), dialogue,
                                                     temperature=temperature,
                                                     model=model, endpoint=endpoint)
        if part_dist:
            distribution += part_dist
            progress.append(asst_progress)

    # normalize distribution
    # This currently results in a `None` column.
    distribution = {k : (v / distribution.total()) for k, v in distribution.items()}
    answer_columns = [f'probability: {answer}' for answer in distribution.keys()]
    answers = tuple(distribution.values())
    answer = max(distribution, key=distribution.get)

    return pd.DataFrame([answers + (answer, progress, full_convo)],
                        index=index, columns=(answer_columns + ['response', 'text', 'full']))

def make_dialogue(gameState, context_name, examples, qualification, show_chart, chart_type):

    if qualification:
        description = make_question_description(gameState, context=None, unit=None)
        question = qual_question(gameState, chart_type)
        dialogue = description + question
    else:
        context = CONTEXTS[context_name]['context']
        unit = CONTEXTS[context_name]['unit']

        description = make_question_description(gameState, context, unit)

        if show_chart:
            if chart_type in ['volume', 'both']:
                description += nbs_qual_description(gameState)
            if chart_type in ['area', 'both']:
                description += mec_qual_description(gameState)

        description += COMPROMISE_Q + '\n'
        propsoal_question = make_question_list(gameState)

        dialogue = description + propsoal_question

    messages = []

    messages.append(('system', SYSTEM))
    messages += examples
    messages.append(('user', QUAL_DIRECTIONS if qualification else HIT_DIRECTIONS))
    messages.append(('user', dialogue))

    return messages

def populate_examples():
    # populate examples
    result = []
    for example in EXAMPLE_DATA:
        if example is None:
            continue

        gameState = pm.VoteGameState.fromArray(example)

        description = make_question_description(gameState, context=None, unit=None)
        question = qual_question(gameState, 'volume')
        model_response = nbs_qual_description(gameState, include_answer=True)
        correct_answer = get_option_letter(gameState, pm.run_nash_bargain(gameState))
        result.append(('user', description + question))
        result.append(('assistant', "Let's think step by step. " + model_response))
        result.append(('user', FOLLOW_UP))
        result.append(('assistant', correct_answer))

        question = qual_question(gameState, 'area')
        model_response = mec_qual_description(gameState, include_answer=True)
        correct_answer = get_option_letter(gameState, pm.run_mec(gameState))

        result.append(('user', description + question))
        result.append(('assistant', "Let's think step by step. " + model_response))
        result.append(('user', FOLLOW_UP))
        result.append(('assistant', correct_answer))
    return result


def setup_api(args):
    # TODO: would be good to verify that model is valid
    # for each...
    if args.anthropic:
        api_key = os.getenv("ANTHROPIC_API_KEY").strip()
        global anthropic
        anthropic = anth.Anthropic(api_key=api_key)
    else:
        # Use the openai framework otherwise
        if args.openai:
            origin = "https://api.openai.com/v1"
            api_key = os.getenv("OPENAI_API_KEY").strip()
        else:
            api_key = os.getenv("TOKEN", "")
            origin = args.origin

        openai.api_key = api_key.strip()
        openai.api_base = origin

if __name__ == "__main__":

    parser = argparse.ArgumentParser(
                    prog='model_experiment',
                    description='Runs the intuition experiment on various LLMs.')
    parser.add_argument('filename', help="The file generated by `scenario_utils.py` to test on.")
    parser.add_argument('--input_directory', default="", help="Where to look for `filename`")
    parser.add_argument('--output_directory', default=OUTPUT_DIR, help="Where to output the results.")
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--qualification', default=False, action='store_true', 
        help=("Whether for not to treat this as a qualification task. That is, whether " +
              "to ask which proposal has the biggest volume or area depending on the " +
              "value of `chart-type`.")
        )
    group.add_argument('--show-chart', default=False, action='store_true',
        help=("Whether or not to describe the charts which would be shown in the human " + 
        "version of this experiment."))
    parser.add_argument('--chart-type', choices=['volume', 'area', 'both'], default="area",
        help="The type of chart to show (or ask for if `qualification`), if any.")
    parser.add_argument('--temperature', default=0, type=int,
        help="The temperature at which to query the model.")
    parser.add_argument('--samples', default=1, type=int, required=False, 
        help=("The number of times to ask the model each individual example. The default " +
        "is once, but asking the same question multiple times (when the temperature is " +
        "greater than zero) approximates getting the token probabilities from the model " +
        "(which are often not available)."))
    parser.add_argument('--endpoint', choices=['chat', 'completion'], default='chat',
        help=("Models are either chat or completion based. Querying a model at the wrong " +
        "endpoint will result in an error."))
    parser.add_argument('--zero-shot', default=False, action='store_true',
        help=("Whether or not to show any in-context examples (pulled from `EXAMPLE_DATA`) " +
        "to show the model. Technically, when `qualification` is not true these examples " +
        "are not in-context as the examples are answers to the qualification task questions. " +
        "Still, the formal models are fully determined by the answers to those questions."))
    parser.add_argument('--model', required=True, help="The model to query.")

    # add_mutually_exclusive_group, flags for openai or anthropic or else provide origin
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--openai', action="store_true", default=False,
        help="Whether to use the openai api and address.")
    group.add_argument('--anthropic', action="store_true", default=False,
        help="Whether to use the anthropic api and address.")
    group.add_argument('--origin', help=("The address to use when using the openai api " +
        "(e.g. with llm-lab)."))

    args = parser.parse_args()


    # TODO: throw these as errors instead
    assert(not args.qualification or args.chart_type != 'both')

    setup_api(args)

    if args.endpoint == 'chat':
        query = query_chat_model
    else:
        query = query_completions_model

    if args.anthropic:
        if args.endpoint == 'chat':
            endpoint = anthropic_chat_with_backoff
        else:
            endpoint = anthropic_completion_with_backoff
    else:
        if args.endpoint == 'chat':
            endpoint = openai_chat_with_backoff
        else:
            endpoint = openai_completion_with_backoff

    filename = os.path.join(args.input_directory, args.filename)
    base_filename = os.path.basename(args.filename)

    file_vars = filename_to_variables(base_filename)

    MAXIMIZE = file_vars['maximize'][0] == 'True'

    num_scenarios = int(file_vars['num-scenarios'][0])

    assert(num_scenarios <= len(ORDER))

    if args.zero_shot:
        examples = []
    else:
        examples = populate_examples()

    if args.temperature == 0:
        args.samples = 1

    # accept a csv output made by `write_to_csv`
    # generate a new csv of questions and options and response from the model

    logging.info(f'Querying {args.model}')

    df = pd.read_csv(filename)

    logging.info(f'Reading from {filename}')

    # Move the data into one df, one scenario each row
    df_exploded = shared_analysis.explode_df(df, num_scenarios)

    results = []
    # TQDM shows a nice progress bar
    for index, row in tqdm.tqdm(df_exploded.iterrows(), total=len(df_exploded)):
        result_s = query_model_df(row, query, endpoint, args.model, args.temperature,
                                    examples,
                                    args.qualification, args.show_chart, args.chart_type,
                                    args.samples)
        results.append(result_s)


    results_series = pd.concat(results)

    # Make sure it is by the same index...
    result_df = df_exploded.merge(results_series, on=['scenario_hash', 'question'])

    run = os.path.splitext(base_filename)[0]

    model_name = args.model.replace('/', '_')

    model_dir = os.path.join(args.output_directory, model_name)

    run_dir = os.path.join(model_dir, run)

    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    if not os.path.exists(run_dir):
        os.makedirs(run_dir)

    output_file = f'temp={args.temperature}_api={args.endpoint}'
    output_file += f'_qualification={args.qualification}_show-charts={args.show_chart}'
    output_file += f'_chart-type={args.chart_type}_samples={args.samples}'
    output_file += f'_zero-shot={args.zero_shot}.csv'

    output = os.path.join(run_dir, output_file)

    logging.info(f'Outputting to {output}')
    result_df.to_csv(output, index=False)
