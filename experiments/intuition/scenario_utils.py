#!./env-parliament/bin/python

'''
```
$ python scenario_utils.py --help               
usage: scenario_utils.py [-h] {scenarios,validate,release,cancel,results,workers} ...

A collection of utilities for running mturk experiments.

positional arguments:
  {scenarios,validate,release,cancel,results,workers}
    scenarios           Makes the scenario csv files.
    validate            Processes the HIT csv. Adds award and qual columns.
    release             Breaks a csv file across various html file for mturk and releases the HITs.
    cancel              Cancels assigned HITs and qualification types which have not been submitted.
    results             Saves the results of the associated HIT if it exists.
    workers             Reports stats on the worker population

options:
  -h, --help            show this help message and exit
$ python scenario_utils.py scenarios --help
usage: scenario_utils.py scenarios [-h] [-m] [--output-directory OUTPUT_DIRECTORY] [--number-scenarios NUMBER_SCENARIOS]
                                   [--num_agents NUM_AGENTS] [--belief-steps BELIEF_STEPS] [--belief-range BELIEF_RANGE]
                                   [--action-steps ACTION_STEPS] [--action-range ACTION_RANGE] [--action-function-logarithmic]
                                   --sample-size SAMPLE_SIZE

options:
  -h, --help            show this help message and exit
  -m, --minimize        Whether to have the models optimize for minimizing values. Maximizing is default.
  --output-directory OUTPUT_DIRECTORY
  --number-scenarios NUMBER_SCENARIOS
                        Number of scenarios to show per HIT.
  --num_agents NUM_AGENTS
                        Number of agents to include in the generated games.
  --belief-steps BELIEF_STEPS
                        Number beliefs (credences) to sample from for each agent.
  --belief-range BELIEF_RANGE
                        The range of beliefs (credences) to sample from.
  --action-steps ACTION_STEPS
                        Number actions (utilities) to sample from for each agent.
  --action-range ACTION_RANGE
                        The range of action (utilities) to sample from.
  --action-function-logarithmic
                        Whether to logarithmically step through the action-range. Default is linear.
  --sample-size SAMPLE_SIZE
                        How many disagreement cases (between NBS and MEC) to generate.
$ python scenario_utils.py validate --help 
usage: scenario_utils.py validate [-h] --filename FILENAME [--qualification]

options:
  -h, --help           show this help message and exit
  --filename FILENAME  The HIT csv as generated by `python scenario_utils.py scenarios`
  --qualification      Whether or not this file is from a qualification task.
$ python scenario_utils.py release --help 
usage: scenario_utils.py release [-h] --filename FILENAME [--output-html OUTPUT_HTML] [--assignments ASSIGNMENTS] [--sandbox]
                                 [--self-assign] --chart-type {area,volume,both}

options:
  -h, --help            show this help message and exit
  --filename FILENAME   The HIT csv as generated by `python scenario_utils.py scenarios`
  --output-html OUTPUT_HTML
                        Whether or not to output the resulting HIT files.
  --assignments ASSIGNMENTS
                        The number of workers to complete each HIT.
  --sandbox             Whether to use the sandbox environment.
  --self-assign         Assigns all of the HITs to the Alexandria account. Defaults to True if --sandbox.
  --chart-type {area,volume,both}
                        The kind of chart to show.
$ python scenario_utils.py cancel --help 
usage: scenario_utils.py cancel [-h] --filename FILENAME [--sandbox] [--self-assign]

options:
  -h, --help           show this help message and exit
  --filename FILENAME  The HIT csv as generated by `python scenario_utils.py scenarios`
  --sandbox            Whether to use the sandbox environment.
  --self-assign        Assigns all of the HITs to the Alexandria account. Defaults to True if --sandbox.
$ python scenario_utils.py results --help
usage: scenario_utils.py results [-h] --filename FILENAME [--sandbox] [--self-assign]

options:
  -h, --help           show this help message and exit
  --filename FILENAME  The HIT csv as generated by `python scenario_utils.py scenarios`
  --sandbox            Whether to use the sandbox environment.
  --self-assign        Assigns all of the HITs to the Alexandria account. Defaults to True if --sandbox.
$ python scenario_utils.py workers --help
usage: scenario_utils.py workers [-h] [--sandbox]

options:
  -h, --help  show this help message and exit
  --sandbox   Whether to use the sandbox environment.
```

Author: Jared Moore (jared@jaredmoore.org)
'''

import argparse
import boto3
import botocore.config
import csv
import datetime
import functools
# import hashlib
import json
import logging
import math
import numpy as np
import os
import pandas as pd
import subprocess
import sys
import tqdm
import xml.etree.ElementTree

import value_aggregation as pm
from shared_analysis import mturk_explode_df, test, control
from utils import filename_to_variables, explode_paginated_call, QUESTION_STRINGS, PROBABILITY_COLUMNS

AGENTS_TEMPLATES = ["apple-üçé", "bee-üêù", "cow-üêÆ"]

SCENARIO_DIRECTORY = 'data/scenarios'

RESULTS_DIR = 'data/results/mturk'

MTURK_DIR = 'mturk'

GENERATED_DIRECTORY = 'generated'
BASE_HTML_TEMPLATE = 'template.html'
HIT_HTML_TEMPLATE = 'health_single.html'
VARIABLES_FILE = 'variables.json'

MTURK_XML = '''<HTMLQuestion xmlns="http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2011-11-11/HTMLQuestion.xsd">
  <HTMLContent><![CDATA[{0}]]>
  </HTMLContent>
  <FrameHeight>0</FrameHeight>
</HTMLQuestion>'''

ALEXANDRIA_MTURK_WORKER_ID = 'A12DWLY5TD1002'

QUAL_TO_IDS = {'moral-parliament-q' : '3XLL3006OQKVHYD6V2X7384QGQD97R',
              'moral-parliament-q-submitted' : '38CFB6RWNPUH9KXTHXRCZVY9D2VVKY',
              'moral-parliament-q-id' : '3F54WXCIFQNTCBXP92FYLEZSZJA56X',
              'moral-parliament-q-assigned-revoked' : '3LJ6LLBDMCOQIH964Y6FTPFERXCA69'}

SANDBOX_QUAL_IDS = {'moral-parliament-q' : '31O6ZS77USI6TUQEOD56C22AGKE7WI',
              'moral-parliament-q-submitted' : '3YZYG5XLRXC7BORQGN96OCCK5V9YH9',
              'moral-parliament-q-id' : '37SHQ51BZ2W0NGSJ8NO1A9F2Y5Q0BV',
              'moral-parliament-q-assigned-revoked' : '3NDLUB5I812BGSTPVXUVBWJN1ID46G'}

QUAL_IDS = None

MTURK_SANDBOX_ENDPOINT = "https://mturk-requester-sandbox.us-east-1.amazonaws.com"

MTURK_DEFAULT_COLS = ['HITId', 'HITTypeId', 'Title', 'Description', 'Keywords',
       'Reward', 'CreationTime', 'MaxAssignments', 'RequesterAnnotation',
       'AssignmentDurationInSeconds', 'AutoApprovalDelayInSeconds',
       'Expiration',
       'AssignmentId', 'WorkerId', 'AssignmentStatus', 'AcceptTime',
       'SubmitTime', 'AutoApprovalTime', 'ApprovalTime', 'RejectionTime',
       'RequesterFeedback', 'Approve', 'Reject']

MTURK_DEFAULT_COLS_DATES = ['CreationTime', 'AcceptTime', 'SubmitTime',
'AutoApprovalTime', 'ApprovalTime', 'RejectionTime']

HIT_STATUS_FILE = 'HIT_status.json'

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

def write_to_csv(hit_list, filename, num_scenarios):

    scenarios = list(range(1, num_scenarios + 1))

    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = []
        for i in scenarios:
            name = f'scenario_{i}'
            fieldnames += [f'scenario_{i}_hash',
                          f'scenario_{i}_json',
                          f'scenario_{i}_mec',
                          f'scenario_{i}_nbs',
                          f'scenario_{i}_mft',
                          f'scenario_{i}_fehr']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for hits in hit_list:
            # row = functools.reduce(lambda x, y: x + y, hit)
            row = {}
            for i in scenarios:
                hit = hits[i - 1]
                new_keys = {key : f'scenario_{i}_{key}' for key in hit.keys()}
                new_hit = {new_keys[key]: value for key, value in hit.items()}
                row.update(new_hit)

                gs = pm.decode_gameState(hit['json'])
                assert(pm.run_mec(gs, none_if_tie=True) == hit['mec'])
                assert(pm.run_nash_bargain(gs, none_if_tie=True) == hit['nbs'])
            writer.writerow(row)

AGGREGATION_FUNCTIONS = {
    'nash' : pm.run_nash_bargain,
    'util' : pm.run_mec,
    'fehr' : lambda **kwargs: pm.run_equality_efficiency(**kwargs, alpha=3/10),
}

def make_scenarios_cmd(args):

    action_range = args.action_range
    if args.minimize:
        action_range = map(lambda x: -x, action_range)

    action_function = np.linspace
    if args.action_function_logarithmic:
        action_function = np.logspace

    agents = AGENTS_TEMPLATES[:args.num_agents]
    actions = ['one', 'two', 'three']

    aggregation_functions = [AGGREGATION_FUNCTIONS[name] for name in args.aggregation_functions]

    # TODO: turn off BELIEF_NORM=100?

    num_scenarios = args.number_scenarios

    if args.greedy_generation:
        hit_list = pm.generate_hits_greedy(agents, actions,
                                            belief_range_steps=args.belief_steps,
                                            belief_range=args.belief_range,
                                            action_range_steps=args.action_steps,
                                            action_range=args.action_range,
                                            action_range_func=action_function,
                                            scenarios_per_hit=num_scenarios,
                                            sample_size=args.sample_size,
                                            aggregation_functions=aggregation_functions,
                                            allow_ties=not args.prevent_ties,
                                            disagrees_only=args.disagrees_only)
    else:
        games = pm.generate_games(agents, actions,
                                  belief_range_steps=args.belief_steps,
                                  belief_range=args.belief_range,
                                  action_range_steps=args.action_steps,
                                  action_range=args.action_range,
                                  action_range_func=action_function,
                                  max_actions=args.max_actions)

        hit_list = pm.generate_hits(games, scenarios_per_hit=num_scenarios,
                                    sample_size=args.sample_size,
                                    aggregation_functions=aggregation_functions,
                                    allow_ties=not args.prevent_ties,
                                    disagrees_only=args.disagrees_only)
    actual_sample = len(hit_list) * num_scenarios / 2

    csv_name = f'maximize={not args.minimize}_num-agents={args.num_agents}'
    csv_name += f'_belief-steps={args.belief_steps}_belief-range={args.belief_range}'
    csv_name += f'_action-steps={args.action_steps}_action-range={args.action_range}'
    csv_name += f'_action-function-log={args.action_function_logarithmic}'
    csv_name += f'_prevent-ties={args.prevent_ties}_agg-functions={args.aggregation_functions}'
    csv_name += f'_disagrees-only={args.disagrees_only}'
    csv_name += f'_num-scenarios={num_scenarios}_sample-size={actual_sample}.csv'
    csv_name = csv_name.replace(' ', '').replace('(', '').replace(')', '')

    write_to_csv(hit_list, os.path.join(args.output_directory, csv_name), num_scenarios)

def generate_xml_files(assignments, num_scenarios):
    os.chdir(MTURK_DIR) # to allow pandoc to work
    logging.info("Generating xml.")
    for i, row in tqdm.tqdm(assignments.iterrows(), total=len(assignments)):

        # This is to pass the json values as parameters to pandoc so they can
        # automatically be populated
        scenario_json = []
        for j in range(1, num_scenarios + 1):
            key = f'scenario_{j}_json'
            value = row[f'Input.{key}']
            scenario_json.append(f'--metadata={key}:{value}')

        # This process itself references the variables file above

        command = ['pandoc', '--standalone', '-f', 'html+raw_html', '-t', 'html',
            '--standalone', '--embed-resources', '--template', BASE_HTML_TEMPLATE,
            HIT_HTML_TEMPLATE, "--metadata=title:HIT"]

        html = subprocess.check_output(command + scenario_json).decode('utf-8')
        assignments.at[i, 'Question'] = MTURK_XML.format(html)
    logging.info("Generated xml")
    os.chdir('..')

def int32hash(string):
    return hash(string) % int(math.pow(2, 31))
#    return int.from_bytes(hashlib.sha256(bytes(str, 'utf-8')).digest()[:4], 'little')

def query_qualified_workers(client):
    # Queries mturk for workers who have passed the qual test but have not yet 
    # submitted or been allocated a HIT.
    # Generates a unique int id for each and returns a list of dicts of worker id, int id

    logging.info("Querying for qualified workers.")

    quals = explode_paginated_call(client.list_workers_with_qualification_type,
                                'Qualifications',
                                QualificationTypeId=QUAL_IDS['moral-parliament-q'])

    logging.info(f"{len(quals)} qualified.")

    logging.info("Querying for submitted workers.")

    submitted = explode_paginated_call(client.list_workers_with_qualification_type,
                                    'Qualifications',
                                    QualificationTypeId=QUAL_IDS['moral-parliament-q-submitted'])
    logging.info(f"{len(submitted)} submitted.")

    logging.info("Querying for assigned workers.")

    has_question = explode_paginated_call(client.list_workers_with_qualification_type,
                                    'Qualifications',
                                    QualificationTypeId=QUAL_IDS['moral-parliament-q-id'])

    logging.info(f"{len(has_question)} assigned HITs.")

    logging.info("Querying for assigned-revoked workers.")

    assigned_revoked = explode_paginated_call(client.list_workers_with_qualification_type,
                                    'Qualifications',
                                    QualificationTypeId=QUAL_IDS['moral-parliament-q-assigned-revoked'])

    logging.info(f"{len(assigned_revoked)} assigned-revoked HITs.")

    quals_df = pd.DataFrame(quals)
    submitted_df = pd.DataFrame(submitted)

    if len(quals_df) <= 0:
        raise ValueError("No workers available!")

    if len(submitted_df) > 0:
        quals_not_submitted = quals_df[(~quals_df['WorkerId'].isin(submitted_df['WorkerId']))]
    else:
        quals_not_submitted = quals_df

    logging.info(f"{len(quals_not_submitted)} after removing submitted.")

    if len(has_question) > 0:
        has_question_df = pd.DataFrame(has_question)
        quals_not_submitted = quals_not_submitted[
        (~quals_not_submitted['WorkerId'].isin(has_question_df['WorkerId']))]

        logging.info(f"{len(quals_not_submitted)} after removing those with questions.")

    available_workers = quals_not_submitted[quals_not_submitted['IntegerValue'] >= 13]

    logging.info(f"{len(available_workers)} after allowing only those with a score >= 13.")

    # I'm pretty sure mturk turns any int into a int32 hence our use of a 
    # unique hash function
    integer_ids = available_workers['WorkerId'].apply(lambda x: int32hash(x))
    integer_ids.name = 'IntegerId'

    # We want only unique values.
    assert(integer_ids.value_counts().max() == 1)

    workers = available_workers.merge(integer_ids, left_index=True, right_index=True)

    if len(assigned_revoked) > 0:
        assigned_revoked_df = pd.DataFrame(assigned_revoked)\
                                .rename(columns={'IntegerValue' : 'AssignedRevoked'})
        workers = workers.merge(assigned_revoked_df, on='WorkerId', how='left')\
                         .fillna(0)\
                         .sort_values(by='AssignedRevoked')
        # All of the workers have passed on the hit, but try to randomize to see if we get others.
        if 0 not in workers['AssignedRevoked'].value_counts():
            workers = workers.sample(frac=1)
        else:
            logging.info(f"{workers['AssignedRevoked'].value_counts()[0]} unassigned workers left!")

    # I don't know if we need to expose the 'AssignedRevoked' column to the consumer
    # so long as they just pick from the front
    workers = workers[['WorkerId', 'IntegerId']].reset_index(drop=True)
    return workers

def scenario_file_to_mturk(in_file, out_file, num_assignments, num_scenarios):
    df = pd.read_csv(in_file)

    new_cols = MTURK_DEFAULT_COLS + ['Answer.feedback', 'Answer.tm']
    rename_cols = {x : f'Input.{x}' for x in list(df.columns)}
    for i in range(1, num_scenarios + 1):
        ans_q_col = f'Answer.q_question-{i}'                                
        new_cols += [ans_q_col,
                        f'{ans_q_col}_attn',
                        f'{ans_q_col}_attn_answer']

    df = df.rename(columns=rename_cols)

    for new_col in new_cols:
        df[new_col] = np.nan

    result = pd.concat([df] * num_assignments, ignore_index=True)

    result.to_csv(out_file, index=False, quoting=csv.QUOTE_ALL)


def get_mturk_client(sandbox):
    global QUAL_IDS
    kwargs = {}
    kwargs['config'] = botocore.config.Config(
       retries = {
          'max_attempts': 10,
          'mode': 'standard'
       }
    )
    if sandbox:
        kwargs['endpoint_url'] = MTURK_SANDBOX_ENDPOINT
        QUAL_IDS = SANDBOX_QUAL_IDS
    else:
        QUAL_IDS = QUAL_TO_IDS
    client = boto3.client('mturk', region_name='us-east-1', **kwargs)
    return client

def mturk_html(args):

    client = get_mturk_client(args.sandbox)

    # Make a new directory for the scenario type crossed with variables

    if args.sandbox:
        args.self_assign = True

    base_filename = os.path.basename(args.filename)
    run = os.path.splitext(base_filename)[0]
    scenario_vars = filename_to_variables(base_filename)
    maximize = scenario_vars['maximize'][0] == 'True'
    num_scenarios = int(scenario_vars['num-scenarios'][0])
    vars_name = f"chart_type={args.chart_type}_maximize={maximize}"

    run_dir = os.path.join(RESULTS_DIR, run)

    if not os.path.exists(run_dir):
        os.makedirs(run_dir)

    this_var_file = vars_name + ".csv"

    results_filename = os.path.join(run_dir, this_var_file)

    # As a backstop load all of the assigned or submitted workers
    assigned_workers = set()
    for file in os.listdir(run_dir):
        filename = os.path.join(run_dir, file)
        _, ext = os.path.splitext(filename)
        if "duplicates" not in file and not os.path.isdir(filename) and ext == "csv":
            assigned_workers |= set(pd.read_csv(filename)['WorkerId'].unique())


    #"duplicates" not in filename and filename != 

    # output a new version of "variables.json" based on the current variables
    # currently overwrites the file, could change this
    # Necessary for generate_xml_files 
    variables = {'chart_type' : args.chart_type, 'maximize' : maximize}
    with open(os.path.join(MTURK_DIR, VARIABLES_FILE), 'w') as variables_file:
        variables_file.write(json.dumps(variables))    

    if not os.path.exists(results_filename):
        scenario_file_to_mturk(args.filename, results_filename, args.assignments, num_scenarios)

    if args.only_generate:
        return

    # Load the previous file, only assign the rows that have not been 
    # filled in.
    results_df = pd.read_csv(results_filename, parse_dates=MTURK_DEFAULT_COLS_DATES)

    # If the HIT has not been assigned
    remaining_assignments = results_df[results_df['HITId'].isnull()].copy()
    assigned = results_df[results_df['HITId'].notnull()].copy()

    logging.info(f"{len(remaining_assignments)} assignments remaining")

    if len(remaining_assignments) < 1:
        print("All assignments assigned.")
        return

    # This is currently generating the html files in a redundant fashion 
    # but it is probably not *too* slow.
    # NB: this modifies remaining_assignments
    generate_xml_files(remaining_assignments, num_scenarios)
    num_assignments = len(remaining_assignments)

    if args.self_assign:
        workers = {'WorkerId' : [ALEXANDRIA_MTURK_WORKER_ID] * num_assignments,
                'IntegerId' : [0] * num_assignments}
        potential_workers = pd.DataFrame(data=workers)
    else:
        potential_workers = query_qualified_workers(client)

    if len(potential_workers) < num_assignments:
        raise ValueError((f'Pool of potential workers ({len(potential_workers)})'
            + f' is too small for number of assignments {num_assignments}.'))

    # Seems not to use exclusive indexing [,] not [,)
    selected_workers = potential_workers.loc[0 :num_assignments - 1]
    assert(selected_workers['WorkerId'].nunique() == num_assignments)
    assert(len(set(selected_workers['WorkerId'].unique()) & assigned_workers) == 0)

    del remaining_assignments['WorkerId'] # This is provided by selected workers
    if 'IntegerId' in remaining_assignments:
        del remaining_assignments['IntegerId'] # This is provided by selected workers
    # Have to reset the index b/c could be middle of csv
    assignments = pd.concat([selected_workers, remaining_assignments.reset_index(drop=True)],
                            axis=1)
    assert(assignments['WorkerId'].nunique() == num_assignments)

    logging.info("Making HITs and qualifying workers.")

    try:
        for i, row in tqdm.tqdm(assignments.iterrows(), total=len(assignments)):
            # TODO: could factor out title, keywords, description
            # TODO: verify make hit invisible
            response = client.create_hit(
                MaxAssignments=1,
                AutoApprovalDelayInSeconds=43200, # One day
                LifetimeInSeconds=43200 * 2,      # Two days
                AssignmentDurationInSeconds=3600, # One hour
                Reward='3',
                Title='When to Compromise Between Groups',
                Keywords='negotiation, compromise, charts',
                Description="We'll ask you to decide on the best compromise option between groups.",
                Question=row['Question'],
                RequesterAnnotation=row['WorkerId'],
                QualificationRequirements=[
                    {
                        'QualificationTypeId': QUAL_IDS['moral-parliament-q-id'],
                        'Comparator': 'EqualTo',
                        'ActionsGuarded' : 'DiscoverPreviewAndAccept',
                        'IntegerValues': [
                            row['IntegerId'],
                        ]
                    }
                ]
            )

            # there should just be one requirement and one integer value
            actual_id = response['HIT']['QualificationRequirements'][0]['IntegerValues'][0]
            if actual_id != row['IntegerId']:
                import pdb; pdb.set_trace()
                logging.error(f"Got id {actual_id} but expected {row['IntegerId']}.")
                response = client.update_expiration_for_hit(
                    HITId=row['HITId'],
                    ExpireAt=expiration
                )
                assignments.at[i, 'Expiration'] = expiration
                break

            # I think there is no need to get a response because mturk
            # throws an error if it fails.
            client.associate_qualification_with_worker(
                QualificationTypeId=QUAL_IDS['moral-parliament-q-id'],
                WorkerId=row['WorkerId'],
                IntegerValue=row['IntegerId'],
                SendNotification=False)

            hit = response['HIT']

            # Populate the columns from the response
            for col in MTURK_DEFAULT_COLS:
                # Had been checking `np.isnan(assignments.at[i, col])`
                # But new hits should overwrite old information
                if col in hit:
                    assignments.at[i, col] = hit[col]

            
            assignments.at[i, 'Expiration'] = hit['Expiration'].replace(tzinfo=None)

    finally:
        del assignments['Question'] # to save space

        combined = pd.concat([assigned, assignments])

        combined.to_csv(results_filename, index=False, quoting=csv.QUOTE_ALL)
        client.close()

def cancel_hits(args):
    results = pd.read_csv(args.filename, parse_dates=MTURK_DEFAULT_COLS_DATES)
    results['Expiration'] = pd.to_datetime(results['Expiration'], format='mixed').dt.tz_localize(None)

    client = get_mturk_client(args.sandbox)

    for i, row in tqdm.tqdm(results.iterrows(), total=len(results)):
        if (not pd.isnull(row['HITId']) and row['Expiration'] > datetime.datetime.now()
            and (pd.isnull(row['AssignmentStatus']) or len(row['AssignmentStatus']) == 0 )):
            expiration = datetime.datetime.now().replace(tzinfo=None)
            response = client.update_expiration_for_hit(
                HITId=row['HITId'],
                ExpireAt=expiration
            )
            results.at[i, 'Expiration'] = expiration

    client.close()
    results.to_csv(args.filename, index=False, quoting=csv.QUOTE_ALL)

# TODO: move this to utils
def parse_mturk_xml_answer(data):
    root = xml.etree.ElementTree.fromstring(data)
    answers = {}
    for child in root:
        answer_id = child[0].text
        answer = child[1].text
        answers[answer_id] = answer
    return answers

def worker_pool_stats(args):
    client = get_mturk_client(args.sandbox)
    potential_workers = query_qualified_workers(client)

def reset_ids(args):

    client = get_mturk_client(args.sandbox)
    has_question = explode_paginated_call(client.list_workers_with_qualification_type,
                                    'Qualifications',
                                    QualificationTypeId=QUAL_IDS['moral-parliament-q-id'])

    logging.info(f"{len(has_question)} remaining with IDs")

    for row in tqdm.tqdm(has_question):
        client.disassociate_qualification_from_worker(
            WorkerId=row['WorkerId'],
            QualificationTypeId=QUAL_IDS['moral-parliament-q-id'])

# TODO: explode args instead of passing in all together?
def check_results(args):

    client = get_mturk_client(args.sandbox)

    results_df = pd.read_csv(args.filename, parse_dates=MTURK_DEFAULT_COLS_DATES)
    released = results_df[results_df['HITId'].notnull()].copy()
    released['Expiration'] = pd.to_datetime(released['Expiration'], format='mixed').dt.tz_localize(None)
    not_released = results_df[results_df['HITId'].isnull()].copy()

    logging.info("Querying for assignment statuses.")

    unfinished = 0
    wrong_submitted = 0

    try:
        for i, row in tqdm.tqdm(released.iterrows(), total=len(released)):
            hit = client.get_hit(HITId=row['HITId'])
            response = client.list_assignments_for_hit(HITId=row['HITId'])
            assignments = response['Assignments']
            assert(len(assignments) <= 1)

            if (len(assignments) == 0 or pd.isnull(assignments[0]['AssignmentStatus']) or 
                    len(assignments[0]['AssignmentStatus']) == 0):
                unfinished += 1
                if row['Expiration'] < datetime.datetime.now():
                    try:
                        client.disassociate_qualification_from_worker(
                            WorkerId=row['WorkerId'],
                            QualificationTypeId=QUAL_IDS['moral-parliament-q-id'])
                    except client.exceptions.RequestError as e:
                        # Presumably this is an error which results from disassociating the same 
                        # id repeatedly from a worker which happens in the sandbox environment
                        if not "This operation can be called with a status of: Granted" in str(e):
                            raise e

                    score = 0
                    try:
                        # Count how many times we have had to revoke tasks from this user.
                        response = client.get_qualification_score(
                            WorkerId=row['WorkerId'],
                            QualificationTypeId=QUAL_IDS['moral-parliament-q-assigned-revoked'])

                        score = response['Qualification']['IntegerValue']

                    except client.exceptions.RequestError as e:
                        if not "You requested a Qualification that does not exist." in str(e):
                            raise e

                    client.associate_qualification_with_worker(
                        WorkerId=row['WorkerId'],
                        QualificationTypeId=QUAL_IDS['moral-parliament-q-assigned-revoked'],
                        IntegerValue=score + 1,
                        SendNotification=False)

                    for col in MTURK_DEFAULT_COLS:
                        released.at[i, col] = np.nan
                continue

            # There should only be one assignment
            assignment = assignments[0]

            if assignment['WorkerId'] != row['WorkerId']:
                logging.error(f"The wrong worker, {assignment['WorkerId']} not {row['RequesterAnnotation']} submitted this HIT.")
                wrong_submitted += 1

            # If the answer is submitted, update the column and qual
            for col in MTURK_DEFAULT_COLS:
                if col in assignment:
                    released.at[i, col] = assignment[col]

            answers = parse_mturk_xml_answer(assignment['Answer'])
            answers = {f'Answer.{k}' : v for k, v in answers.items()}
            for answer_key, answer in answers.items():
                released.at[i, answer_key] = answer

            try:
                client.associate_qualification_with_worker(
                    QualificationTypeId=QUAL_IDS['moral-parliament-q-submitted'],
                    WorkerId=assignment['WorkerId'],
                    IntegerValue=1,
                    SendNotification=False)
                client.disassociate_qualification_from_worker(
                            WorkerId=assignment['WorkerId'],
                            QualificationTypeId=QUAL_IDS['moral-parliament-q-id'])
            except client.exceptions.RequestError as e:
                if ("This operation can be called with a status of: Granted" not in str(e)):                
                    raise e
    finally:
        logging.info(f"{unfinished} assignments unfinished out of {len(released)} total.")
        logging.info(f"{wrong_submitted} wrong submissions.")
        # TODO: should probably put this all in a finally block
        combined = pd.concat([not_released, released])

        combined.to_csv(args.filename, index=False, quoting=csv.QUOTE_ALL)
        client.close()

def score(row):
    score = 0
    for question, answer in qualification_answers.items():
        score += row[question] == answer
    return score

def process_mturk(args):
    df = pd.read_csv(args.filename)

    # TODO: if it is a results file, query the web directly?
    # also should just upload these automatically for all files

    with open("mturk/qualification_answers.json", 'r') as qualJson:
        qualification_answers = json.load(qualJson) 
        qualification_answers = {f'Answer.{k}' : v for k, v in qualification_answers.items()}

    if args.qualification:
        # Change how success is judged
        client = get_mturk_client(False)
        perfect_scores = 0
        for _, row in tqdm.tqdm(df.iterrows(), total=len(df)):
            score = 0
            for question, answer in qualification_answers.items():
                score += row[question] == answer
            if score == 13:
                perfect_scores += 1
            try:
                client.associate_qualification_with_worker(
                    QualificationTypeId=QUAL_IDS['moral-parliament-q'],
                    WorkerId=row['WorkerId'],
                    IntegerValue=score,
                    SendNotification=False)
            except client.exceptions.RequestError as e:
                if not "This operation can be called with a status of: Granted" in str(e):
                    raise e
                else:
                    logging.info("Qual already associated")
        logging.info(f"{perfect_scores} perfect scores")

    else:
        # TODO: This flow is no longer needed
        is_first = df['WorkerId'].duplicated(keep='first')
        counts = is_first.value_counts()
        count = 0 if True not in counts else counts[True]
        logging.info(f"Duplicates: {count}, % {count / len(is_first) * 100:.2f}")
        approved = is_first.apply(lambda x: '' if x else 'x')
        approved.name = 'Approve'
        rejected = is_first.apply((lambda x: 'duplicate' if x else ''))
        rejected.name = 'Reject'
        submitted = pd.Series([1] * len(df), index=pd.RangeIndex(len(df)))
        submitted.name = 'UPDATE-moral-parliament-q-submitted'
        del df['Approve']
        del df['Reject']
        if 'UPDATE-moral-parliament-q-submitted' in df:
            del df['UPDATE-moral-parliament-q-submitted']
        df = df.merge(pd.concat([approved, rejected, submitted], axis=1),
                        left_index=True, right_index=True)

    df.to_csv(args.filename, index=False, quoting=csv.QUOTE_ALL)

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='A collection of utilities for running mturk experiments.')

    subparsers = parser.add_subparsers(dest='cmd', required=True)
    
    make_template_parser = subparsers.add_parser("scenarios",
        help='Makes the scenario csv files.')
    make_template_parser.add_argument('-m', '--minimize', default=False, action='store_true',
        help="Whether to have the models optimize for minimizing values. Maximizing is default.")
    make_template_parser.add_argument('--output-directory', default=SCENARIO_DIRECTORY)
    make_template_parser.add_argument('--number-scenarios', type=int, default=3,
        help="Number of scenarios to show per HIT.")
    make_template_parser.add_argument('--num-agents', default=3, type=int,
        help="Number of agents to include in the generated games.")
    make_template_parser.add_argument('--belief-steps', default=1, type=int,
        help="Number beliefs (credences) to sample from for each agent.")
    make_template_parser.add_argument('--belief-range', default=(3,3), nargs='+', type=float,
        help="The range of beliefs (credences) to sample from.")
    make_template_parser.add_argument('--action-steps', default=3, type=int,
        help="Number actions (utilities) to sample from for each agent.")
    make_template_parser.add_argument('--max-actions', type=int,
        help="Number of action combinations to sample to prevent combinatorial explosion.")
    make_template_parser.add_argument('--action-range', default=(1,101), type=tuple,
        help="The range of action (utilities) to sample from.")
    make_template_parser.add_argument('--action-function-logarithmic', default=False,
        action="store_true",
        help="Whether to logarithmically step through the action-range. Default is linear.")
    make_template_parser.add_argument('--sample-size', type=int, required=True,
        help="How many disagreement cases (between NBS and MEC) to generate.")
    make_template_parser.add_argument('--prevent-ties', action='store_true',
        default=False,  help="Whether to prevent ties.")
    make_template_parser.add_argument('--disagrees-only', action='store_true',
        default=False,  help="Whether to only store disagreements.")
    make_template_parser.add_argument('--greedy-generation', action='store_true',
        default=False,  help="Whether to greedily instatiate scenarios and not enumerate them all.")
    make_template_parser.add_argument('--aggregation-functions', nargs='+', type=str,
        default=['nash', 'util'],
        help="Which functions to compare between.")
    make_template_parser.set_defaults(func=make_scenarios_cmd)


    process_mturk_parser = subparsers.add_parser("validate", 
        help='Processes the HIT csv. Adds award and qual columns.')
    process_mturk_parser.add_argument('--filename', required=True,
        help="The HIT csv as generated by `python scenario_utils.py scenarios`")
    process_mturk_parser.add_argument('--qualification', default=False, action='store_true',
        help="Whether or not this file is from a qualification task.")
    process_mturk_parser.set_defaults(func=process_mturk)


    make_mturk_html = subparsers.add_parser("release",
        help='Breaks a csv file across various html file for mturk and releases the HITs.')
    make_mturk_html.add_argument('--only-generate', default=False, action="store_true",
        help="Only generate the scenario split file don't make the html files or release them.")
    make_mturk_html.add_argument('--filename', required=True,
        help="The HIT csv as generated by `python scenario_utils.py scenarios`")
    make_mturk_html.add_argument('--output-html', default=False,
        help="Whether or not to output the resulting HIT files.")
    make_mturk_html.add_argument('--assignments', default=3,
        help="The number of workers to complete each HIT.")
    make_mturk_html.add_argument('--sandbox', default=False, action="store_true",
        help="Whether to use the sandbox environment.")
    make_mturk_html.add_argument('--self-assign', default=False, action="store_true",
        help="Assigns all of the HITs to the Alexandria account. Defaults to True if --sandbox.")
    make_mturk_html.set_defaults(func=mturk_html)
    make_mturk_html.add_argument('--chart-type', choices=["area", "volume", "both"], required=True,
        help="The kind of chart to show.")
    # TODO: store the released HIT IDs in a json file?

    cancel = subparsers.add_parser("cancel",
        help='Cancels assigned HITs and qualification types which have not been submitted.')
    cancel.add_argument('--filename', required=True,
        help="The HIT csv as generated by `python scenario_utils.py scenarios`")
    cancel.add_argument('--sandbox', default=False, action="store_true",
        help="Whether to use the sandbox environment.")
    cancel.add_argument('--self-assign', default=False, action="store_true",
        help="Assigns all of the HITs to the Alexandria account. Defaults to True if --sandbox.")
    cancel.set_defaults(func=cancel_hits)

    results = subparsers.add_parser("results",
        help='Saves the results of the associated HIT if it exists.')
    results.add_argument('--filename', required=True,
        help="The HIT csv as generated by `python scenario_utils.py scenarios`")
    results.add_argument('--sandbox', default=False, action="store_true",
        help="Whether to use the sandbox environment.")
    results.add_argument('--self-assign', default=False, action="store_true",
        help="Assigns all of the HITs to the Alexandria account. Defaults to True if --sandbox.")
    results.set_defaults(func=check_results)

    workers = subparsers.add_parser("workers",
        help='Reports stats on the worker population')
    workers.add_argument('--sandbox', default=False, action="store_true",
        help="Whether to use the sandbox environment.")
    workers.set_defaults(func=worker_pool_stats)

    workers = subparsers.add_parser("reset",
        help='Clears out all of the assigned worker qualification ids')
    workers.add_argument('--sandbox', default=False, action="store_true",
        help="Whether to use the sandbox environment.")
    workers.set_defaults(func=reset_ids)
    
    args = parser.parse_args()
    args.func(args)
